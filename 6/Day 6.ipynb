{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.19901693177241017\n",
      "ROUGE-L score: 0.5755707158816794\n",
      "--\n",
      "\t The Cricketers is a family-friendly low rated  Chinese restaurant coffee shop  near The Portland Arms .\n",
      "\t Fitzbillies is a non family-friendly 1 out of 5 rated high priced  Chinese restaurant coffee shop in city centre .\n",
      "\t The Golden Palace is a 1 out of 5 rated high priced  Chinese restaurant coffee shop in city centre .\n",
      "\t The Punter is a family-friendly 5 out of 5 rated  restaurant in city centre .\n",
      "\t Clowns is a low rated  English restaurant coffee shop  near Clare Hall in city centre .\n",
      "\t Fitzbillies is a non family-friendly 1 out of 5 rated moderate priced  Chinese restaurant coffee shop in riverside .\n",
      "\t The Mill is a moderate priced  English restaurant coffee shop  near The Sorrento in city centre .\n",
      "\t The Wrestlers is a family-friendly 3 out of 5 rated  restaurant  near The Sorrento .\n",
      "\t Browns Cambridge is a non family-friendly low rated  Chinese restaurant coffee shop  near Crowne Plaza Hotel in riverside .\n",
      "\t Fitzbillies is a family-friendly 5 out of 5 rated cheap priced  English restaurant coffee shop in city centre .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ass6utils import read_file, score, MeaningRepresentation, bleu_single\n",
    "import random\n",
    "\n",
    "meaning_representations, references = read_file('devset.csv')\n",
    "\n",
    "def generate(mr: MeaningRepresentation) -> str:\n",
    "    return '{} is a {} {}.'.format(\n",
    "        mr.name,\n",
    "        mr.food,\n",
    "        mr.eat_type,\n",
    "    )\n",
    "\n",
    "def no_none(word, prefix='', replace=None, postfix='', default = None):\n",
    "    if default:\n",
    "        word = default\n",
    "    if word == None:\n",
    "        return ''\n",
    "    if replace:\n",
    "        if word == 'no':\n",
    "            return prefix + 'non ' + replace + postfix + ' '\n",
    "        else:\n",
    "            return prefix + replace + postfix + ' '\n",
    "    return prefix + word + postfix + ' '\n",
    "\n",
    "def generate(mr: MeaningRepresentation) -> str:\n",
    "    return f'{no_none(mr.name)}is a {no_none(mr.family_friendly, replace=\"family-friendly\")}{no_none(mr.customer_rating, postfix=\" rated\")}{no_none(mr.price_range, postfix=\" priced\")} {no_none(mr.food)}restaurant {no_none(mr.eat_type)}{no_none(mr.near, prefix=\" near \")}{no_none(mr.area, prefix=\"in \")}.'\n",
    "\n",
    "score(generate, meaning_representations, references)\n",
    "print('--')\n",
    "for _ in range(10):\n",
    "    print('\\t', generate(random.choice(meaning_representations)))\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On a basic level the task is not too difficult if the goal is to just generate that x is a z type thing in y.\n",
    "2. Well the scores by themselves don't really mean anything but when they are combined with a task such as this and to other systems they have the ability to somewhat rank the systems but they don't really explain exactly how good a human would think the system is.\n",
    "3. The devset represents very formulaic type of text that could be easily represented as rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The meteor score did not work for me, so I don't get the meteor scores. The default generates BLEU score of 0.03 and rouge score of 0.38.\n",
    "2. These are very poor results compared to the baseline.\n",
    "3. Main problem is the None outputs when the word does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some rules I got the following scores and sentences.\n",
    "\n",
    "BLEU score: 0.19901693177241017\n",
    "ROUGE-L score: 0.5755707158816794\n",
    "\n",
    "\n",
    "\t The Wrestlers is a non family-friendly 5 out of 5 rated  restaurant  near The Sorrenato .\n",
    "\t Cocum is a non family-friendly 1 out of 5 rated moderate priced  English restaurant coffee shop .\n",
    "\t The Eagle is a non family-friendly high rated £20-25 priced  English restaurant coffee shop  near Burger King in riverside .\n",
    "\t Fitzbillies is a non family-friendly 5 out of 5 rated cheap priced  Chinese restaurant coffee shop in riverside .\n",
    "\t The Wrestlers is a non family-friendly less than £20 priced  English restaurant coffee shop  near Raja Indian Cuisine in riverside .\n",
    "\t Fitzbillies is a non family-friendly average rated cheap priced  Chinese restaurant coffee shop in riverside .\n",
    "\t Clowns is a high rated  English restaurant coffee shop  near Clare Hall in riverside .\n",
    "\t The Eagle is a family-friendly low rated less than £20 priced  Chinese restaurant coffee shop  near Burger King in riverside .\n",
    "\t Cotto is a 3 out of 5 rated moderate priced  Chinese restaurant coffee shop  near The Portland Arms in city centre .\n",
    "\t Fitzbillies is a family-friendly 1 out of 5 rated high priced  English restaurant coffee shop in city centre .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_none(word, prefix='', replace=None, postfix=''):\n",
    "    if word == None:\n",
    "        return ''\n",
    "    if replace:\n",
    "        return prefix + replace + postfix + ' '\n",
    "    return prefix + word + postfix + ' '\n",
    "\n",
    "def generate(mr: MeaningRepresentation) -> str:\n",
    "    return f'{no_none(mr.name)}is a {no_none(mr.family_friendly, replace=\"family-friendly\")}{no_none(mr.customer_rating, postfix=\" rated\")}{no_none(mr.price_range, postfix=\" priced\")} {no_none(mr.food)}{no_none(mr.eat_type)}{no_none(mr.near, prefix=\" near \")}{no_none(mr.area, prefix=\"in \")}.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t The Eagle is a family-friendly 3 out of 5 rated moderate priced  English coffee shop  near Burger King in riverside .\n"
     ]
    }
   ],
   "source": [
    "print('\\t', generate(random.choice(meaning_representations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5\n",
    "\n",
    "1. It would not be hard to add some various phrases, it would only take a bit of time.\n",
    "2. Would be difficult to deal with the various conjugations of the words.\n",
    "3. This is a very simple rule based system.\n",
    "4. Well given the restricted scope it was as difficult as was expected, a better and more complex system would require a lot more thinking\n",
    "5. If we only want to extract the information with no significant 'new' descriptions classical methods will work fine enough. Neural network will generate more 'novel' descriptions and likely more natural sounding examples. \n",
    "6. The rouge score got to about same level as the worst in the table, but BLEU score is not even close.\n",
    "7. Seq2seq models seem to be quite good in this task, which is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7\n",
    "\n",
    "1. Was this an interesting description, does it contain all the interesting information.\n",
    "2. Send person to the restaurant and see if the description was correct.\n",
    "3. They topped both categories.\n",
    "4. Well if we are trying to get people to go to restaurants, then naturalness, if we want to give honest evaluations, then optimize for quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6\n",
    "\n",
    "When creating the improved version, I added restaurant before eat_type field in all things. This increases BLEU score by about 0.007, since likely with the restaurant we get some good n-grams. Since the BLEU score only checks the existence of n-grams, it has no real \"understanding\" for whether sentences make sense, as a human would evaluate sentences, since the phrase 'restaurant coffee shop' is not really very good but for BLEU it is good since it likely scores some n-grams and gets points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bleu_single() missing 1 required positional argument: 'reference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-7d441e61874c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: bleu_single() missing 1 required positional argument: 'reference'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
