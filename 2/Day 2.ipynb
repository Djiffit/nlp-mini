{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One morning I shot an elephant in my pajamas.', \"How he got into my pajamas I'll never know.\"]\n",
      "['One', 'morning', 'I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas', '.']\n",
      "[('One', 'CD'), ('morning', 'NN'), ('I', 'PRP'), ('shot', 'VBP'), ('an', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NN'), ('.', '.')]\n",
      "['elephant', 'go', 'go', 'going', 'went', 'gone']\n",
      "{'ve', 'after', 'your', \"haven't\", 'ours', 'shouldn', \"you're\", 'as', 'just', \"you've\", 'because', \"shouldn't\", 'there', 'herself', 'won', 'until', 'for', 'shan', 'themselves', 'before', 'y', 'about', \"aren't\", 'above', 'd', 'once', 'when', 'more', 'very', 'yourself', 'out', 'below', 'those', \"didn't\", \"hadn't\", 'weren', 'at', 'll', 'with', 'ain', 'between', 'where', 'by', 'nor', 't', 'myself', 'their', 'both', \"wasn't\", 'these', 'few', 'some', 'doesn', 'needn', 'while', 'what', 'against', 'all', 'don', 'that', \"shan't\", 'you', 'not', 'am', 'o', \"hasn't\", \"that'll\", 'here', \"doesn't\", 'been', 'off', 'each', 'whom', 'her', \"you'd\", 'has', 'or', 'again', 'its', 'is', 's', 'down', 'further', 'it', 'being', 'my', 'through', 'most', 'they', \"mightn't\", \"you'll\", 'other', 'under', 'so', \"wouldn't\", \"won't\", 'an', 'why', 'mustn', 'i', 'such', 'and', 'then', \"it's\", 'does', 'hasn', 'up', 'own', \"needn't\", 'were', 'didn', \"isn't\", 'ourselves', \"don't\", 'our', 'aren', 'of', 'mightn', 'will', 'yours', 'them', \"mustn't\", 'ma', 'himself', 'only', 'the', 'yourselves', 'couldn', 'on', \"couldn't\", 'if', 'should', 'him', 'too', 'itself', 'from', 'than', 'was', 'm', 'no', \"she's\", 'doing', 'do', 'a', 'but', 'wouldn', 'can', 'how', 'this', \"weren't\", 'he', 'his', 'me', 'wasn', 'during', 'which', \"should've\", 'into', 'who', 'hadn', 'theirs', 'isn', 'hers', 'same', 'having', 'over', 'are', 'haven', 'to', 're', 'did', 'now', 'she', 'in', 'we', 'have', 'any', 'be', 'had'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # used to lemmatize words.\n",
    "\n",
    "text = \"One morning I shot an elephant in my pajamas. How he got into my pajamas I'll never know.\" # by Groucho Marx\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "\n",
    "words = word_tokenize(sentences[0])\n",
    "print(words)\n",
    "\n",
    "pos = pos_tag(words)\n",
    "print(pos)\n",
    "\n",
    "print([lemmatizer.lemmatize(w) for w in ['elephants', 'go', 'goes', 'going', 'went', 'gone']])\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sentence tokenization is splitting multiple sentences into separate sentence entities.\n",
    "2. Splitting splitting a sentence into separate token entities i.e. words, commas etc.\n",
    "3. It classifies words tokens as the parts of speech that they represent. It can be sometimes helpful to find some specific parts of a sentence and it can help in other NLP tasks.\n",
    "4. Its goal is to join separate types of a single word into the same token, i.e. dogs, dog's and dog are the same dog lemma. Now if you want to find all references to the dog token it can be done easily as all of the references of dog are in the same lemmatized form.\n",
    "5. They are common words that do not really convey much meaning of the sentence and as such can sometimes be ignored when analyzing texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own NLP pipeline (a function named process_text(text)) that takes a paragraph as input, and splits the paragraph into sentences, applies word tokenization, POS tagging and lemmatization on all words. The function should return a list containing the processed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['Build', 'your', 'own', 'NLP', 'pipeline', '(', 'a', 'function', 'named', 'process_text', '(', 'text', ')', ')', 'that', 'takes', 'a', 'paragraph', 'as', 'input', ',', 'and', 'splits', 'the', 'paragraph', 'into', 'sentences', ',', 'applies', 'word', 'tokenization', ',', 'POS', 'tagging', 'and', 'lemmatization', 'on', 'all', 'words', '.'], ['The', 'function', 'should', 'return', 'a', 'list', 'containing', 'the', 'processed', 'sentences', '.']], [[('Build', 'VB'), ('your', 'PRP$'), ('own', 'JJ'), ('NLP', 'NNP'), ('pipeline', 'NN'), ('(', '('), ('a', 'DT'), ('function', 'NN'), ('named', 'VBN'), ('process_text', 'NN'), ('(', '('), ('text', 'NN'), (')', ')'), (')', ')'), ('that', 'WDT'), ('takes', 'VBZ'), ('a', 'DT'), ('paragraph', 'NN'), ('as', 'IN'), ('input', 'NN'), (',', ','), ('and', 'CC'), ('splits', 'VBZ'), ('the', 'DT'), ('paragraph', 'NN'), ('into', 'IN'), ('sentences', 'NNS'), (',', ','), ('applies', 'NNS'), ('word', 'NN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), ('and', 'CC'), ('lemmatization', 'NN'), ('on', 'IN'), ('all', 'DT'), ('words', 'NNS'), ('.', '.')], [('The', 'DT'), ('function', 'NN'), ('should', 'MD'), ('return', 'VB'), ('a', 'DT'), ('list', 'NN'), ('containing', 'VBG'), ('the', 'DT'), ('processed', 'JJ'), ('sentences', 'NNS'), ('.', '.')]], [['Build', 'your', 'own', 'NLP', 'pipeline', '(', 'a', 'function', 'named', 'process_text', '(', 'text', ')', ')', 'that', 'take', 'a', 'paragraph', 'a', 'input', ',', 'and', 'split', 'the', 'paragraph', 'into', 'sentence', ',', 'applies', 'word', 'tokenization', ',', 'POS', 'tagging', 'and', 'lemmatization', 'on', 'all', 'word', '.'], ['The', 'function', 'should', 'return', 'a', 'list', 'containing', 'the', 'processed', 'sentence', '.']])\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "def process_text(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    words = [word_tokenize(sent) for sent in sents]\n",
    "    pos = [pos_tag(sent) for sent in words]\n",
    "    lem = [[lemmatizer.lemmatize(word) for word in sent] for sent in words]\n",
    "    \n",
    "    assert(len(words) == len(pos) and len(words) == len(lem))\n",
    "    assert(len(words[0]) == len(pos[0]) and len(words[0]) == len(lem[0]))\n",
    "    \n",
    "    return (words, pos, lem)\n",
    "    \n",
    "text = 'Build your own NLP pipeline (a function named process_text(text)) that takes a paragraph as input, and splits the paragraph into sentences, applies word tokenization, POS tagging and lemmatization on all words. The function should return a list containing the processed sentences.'\n",
    "\n",
    "process = process_text(text)\n",
    "\n",
    "print(process)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function (filter_text(text)) that uses process_text(text) to process a paragraph and then removes stop words and words that are not verbs, adjectives or nouns (for descriptions of POS tags, read this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Implement', 'function', 'filter_text', 'text', 'uses', 'process_text', 'text', 'process', 'paragraph', 'removes', 'stop', 'words', 'words', 'verbs', 'adjectives', 'nouns', 'descriptions', 'POS', 'tags', 'read', 'second', 'sentence', 'wow', 'everyone', 'thing', 'working', 'masterpiece', 'software']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "accepted_pos = re.compile('(NN.?.?|VB.?|JJ.?)')\n",
    "\n",
    "def filter_text(text):\n",
    "    words, pos, lem = process_text(text)\n",
    "    filtered = []\n",
    "    \n",
    "    for sind, sent in enumerate(lem):\n",
    "        for wind, word in enumerate(sent):\n",
    "            if word not in stopWords and accepted_pos.match(pos[sind][wind][1]):\n",
    "                filtered += [words[sind][wind]]\n",
    "                \n",
    "    return filtered\n",
    "\n",
    "text = 'Implement a function (filter_text(text)) that uses process_text(text) to process a paragraph and then removes stop words and words that are not verbs, adjectives or nouns (for descriptions of POS tags, read this). Here is a second sentence to wow everyone how well this thing is truly working. What a masterpiece of software.' \n",
    "print(filter_text(text))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB VB\n",
      "DET DT\n",
      "NOUN NN\n",
      "PUNCT -LRB-\n",
      "NOUN NN\n",
      "PUNCT -RRB-\n",
      "PUNCT -RRB-\n",
      "DET WDT\n",
      "VERB VBZ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en\") \n",
    "    \n",
    "def spacy_parser(text):\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            print(token.text, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dependency parsing attempts to find the dependency relations between the words in a sentence. For example, that in  \"I am eating\" I and am are dependent on eating. POS tagging only wants to find out the tag for a single word.\n",
    "2. The pos_ attribute is a simplified version of the tag_ attribute, both describe the words part-of-speech role.\n",
    "3. A chunk is a collection of words that describe a single entity for example in the form the -adjective- -adjective- noun.\n",
    "4. NER is a process whose goal is to recognize different entities in the text, for example organizations, products, money etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON PRP nsubj\n",
      "have VERB VBP ROOT\n",
      "a DET DT det\n",
      "dog NOUN NN dobj\n",
      "None\n",
      "([['I', 'have', 'a', 'dog']], [[('I', 'PRP'), ('have', 'VBP'), ('a', 'DT'), ('dog', 'NN')]], [['I', 'have', 'a', 'dog']])\n"
     ]
    }
   ],
   "source": [
    "def parse_compare(text):\n",
    "    print(spacy_parser(text))\n",
    "    print(process_text(text))\n",
    "\n",
    "parse_compare('I have a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finger NOUN NN nsubj\n",
      "licking VERB VBG ROOT\n",
      "good ADJ JJ acomp\n",
      ". PUNCT . punct\n",
      "None\n",
      "([['Finger', 'licking', 'good', '.']], [[('Finger', 'NNP'), ('licking', 'VBG'), ('good', 'JJ'), ('.', '.')]], [['Finger', 'licking', 'good', '.']])\n"
     ]
    }
   ],
   "source": [
    "parse_compare('Finger licking good.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finger PROPN NNP compound\n",
      "Lickin PROPN NNP ROOT\n",
      "' PUNCT '' case\n",
      "Good PROPN NNP amod\n",
      "None\n",
      "([['Finger', 'Lickin', \"'\", 'Good']], [[('Finger', 'NNP'), ('Lickin', 'NNP'), (\"'\", 'POS'), ('Good', 'JJ')]], [['Finger', 'Lickin', \"'\", 'Good']])\n"
     ]
    }
   ],
   "source": [
    "parse_compare('Finger Lickin\\' Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_compare('Think different.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK and Stanford give adjective tag to 'different' in think different when it is in lower case and consider it to be a noun when it is uppercase. Spacy thinks it is an adjective in both cases. Spacy thinks that Finger Lickin' Good is NNP NNP NNP, NLTK gives NNP NNP JJ and Stanford tool gives NN VBG JJ, which is the same as the original Finger licking good for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, xmltodict, pickle, os\n",
    "import numpy as np\n",
    "\n",
    "def totally_pun_word(word):\n",
    "    res = requests.get(f'https://api.datamuse.com/words?sl={word}').json()\n",
    "    rand = np.random.randint(0, len(res))\n",
    "    return res[rand]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "arpabet = cmudict.dict()\n",
    "def pronounce(word):\n",
    "    return arpabet[word.lower()][0] if word.lower() in arpabet else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "distance = editdistance.eval(pronounce('pi'), pronounce('pie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juris park\n",
      "Jurassic poar\n",
      "Jurassic purk\n",
      "gierig park\n",
      "Life of paye\n",
      "fluff of Pi\n",
      "Life of piy\n",
      "lerf of Pi\n",
      "Life of pye\n",
      "Game of thomes\n",
      "Game of arens\n",
      "Game of careens\n",
      "Game of crohns\n",
      "laury of the Rings\n",
      "lardy of the Rings\n",
      "Lord of the rez\n",
      "loud of the Rings\n"
     ]
    }
   ],
   "source": [
    "def make_punny(text):\n",
    "    fil = filter_text(text)\n",
    "    rand = np.random.randint(0, len(fil))\n",
    "    replace = fil[rand]\n",
    "    replacer = totally_pun_word(fil[rand])\n",
    "    text = text.replace(replace, replacer)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "print(make_punny('Jurassic park'))\n",
    "print(make_punny('Jurassic park'))\n",
    "print(make_punny('Jurassic park'))\n",
    "print(make_punny('Jurassic park'))\n",
    "print(make_punny('Life of Pi'))\n",
    "print(make_punny('Life of Pi'))\n",
    "print(make_punny('Life of Pi'))\n",
    "print(make_punny('Life of Pi'))\n",
    "print(make_punny('Life of Pi'))\n",
    "print(make_punny('Game of Thrones'))\n",
    "print(make_punny('Game of Thrones'))\n",
    "print(make_punny('Game of Thrones'))\n",
    "print(make_punny('Game of Thrones'))\n",
    "print(make_punny('Lord of the Rings'))\n",
    "print(make_punny('Lord of the Rings'))\n",
    "print(make_punny('Lord of the Rings'))\n",
    "print(make_punny('Lord of the Rings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
