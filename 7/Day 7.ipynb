{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = ['Wage conflict in retail business grows',\n",
    "\t\t\t 'Higher wage for cafeteria employees',\n",
    "\t\t\t 'Retailing Wage Dispute Expands',\n",
    "\t\t\t 'Train Crash Near Petershausen',\n",
    "\t\t\t 'Five Deaths in Crash of Police Helicopter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [list(map(lemmatizer.lemmatize, filter(lambda x: x not in stops, word_tokenize(doc.lower())))) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retail': 0, 'conflict': 1, 'business': 2, 'death': 3, 'higher': 4, 'crash': 5, 'dispute': 6, 'grows': 7, 'train': 8, 'near': 9, 'retailing': 10, 'helicopter': 11, 'wage': 12, 'expands': 13, 'police': 14, 'cafeteria': 15, 'five': 16, 'employee': 17, 'petershausen': 18}\n"
     ]
    }
   ],
   "source": [
    "word2ind = {}\n",
    "ind2word = []\n",
    "\n",
    "for sent in docs:\n",
    "    for word in sent:\n",
    "        ind2word.append(word)\n",
    "    \n",
    "ind2word = set(ind2word)\n",
    "\n",
    "for ind, word in enumerate(ind2word):\n",
    "    word2ind[word] = ind\n",
    "    \n",
    "print(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0.]] (5, 19)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mat = np.zeros((len(documents), len(word2ind)))\n",
    "for ind, doc in enumerate(docs):\n",
    "    for word in doc:\n",
    "        mat[ind, word2ind[word]] += 1\n",
    "        \n",
    "print(mat, mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stops, tokenizer=word_tokenize)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "counts = X.toarray()  # Get the doc-term count matrix\n",
    "dt = counts > 0       # Convert to a binary matrix\n",
    "doc_term_mat = dt * 1 # If you prefer, represent as 1s and 0s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices have same absolute value, but are different since word indexes are not the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document product [[2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "Normalize product [[0.33333333]\n",
      " [0.2       ]\n",
      " [0.25      ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "query_vec = (vectorizer.transform(['retail']).toarray() + vectorizer.transform(['wage']).toarray()).T\n",
    "\n",
    "print('Document product', doc_term_mat.dot(query_vec))\n",
    "\n",
    "normalize = (doc_term_mat.T / np.array([len(d.split()) for d in documents])).T\n",
    "\n",
    "print('Normalize product', normalize.dot(query_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing breaks the tie between the two documents, but the most 'important' document still remains at the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=stops, tokenizer=word_tokenize)\n",
    "tf_X = tf_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "retail_vec = tf_vectorizer.transform(['retail']).toarray()\n",
    "wage_vec = tf_vectorizer.transform(['wage']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = ((tf_X.dot((retail_vec + wage_vec).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wage conflict in retail business grows  -----  Higher wage for cafeteria employees\n",
      " Are similar with cos of  0.22360679774997896\n",
      "Wage conflict in retail business grows  -----  Retailing Wage Dispute Expands\n",
      " Are similar with cos of  0.22360679774997896\n",
      "Higher wage for cafeteria employees  -----  Retailing Wage Dispute Expands\n",
      " Are similar with cos of  0.25\n",
      "Train Crash Near Petershausen  -----  Five Deaths in Crash of Police Helicopter\n",
      " Are similar with cos of  0.22360679774997896\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "doc_count = len(documents)\n",
    "doc_list = [i for i in range(doc_count)]\n",
    "doc_pairs = list(itertools.combinations(doc_list, 2))\n",
    "\n",
    "for aa, bb in doc_pairs:\n",
    "    a,b = doc_term_mat[aa], doc_term_mat[bb]\n",
    "    sim = (np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    if sim > 0:\n",
    "        print(documents[aa], ' ----- ', documents[bb])\n",
    "        print(' Are similar with cos of ', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.42224214]\n",
      " [0.37410477]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "new_docs = [\n",
    "    'Plane crash in Baden-Wuerttemberg',          # Doc 3a\n",
    "\t'The weather'                             # Doc 3b\n",
    "]\n",
    "\n",
    "vec = np.zeros(tf_vectorizer.transform(['ewr']).toarray().shape)\n",
    "for doc in new_docs:\n",
    "    for word in doc.split():\n",
    "        vec += tf_vectorizer.transform([word]).toarray()\n",
    "    similarity = tf_X.dot(vec.T)\n",
    "    print(similarity)\n",
    "    vec = np.zeros(tf_vectorizer.transform(['ewr']).toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(filename):\n",
    "    articles = []\n",
    "    text = open(filename,'r').read().split()\n",
    "    index_start = list(np.where(np.array(text)==\"<DOC\")[0])\n",
    "    for i in range(len(index_start)-1):\n",
    "        start_art = index_start[i]+2\n",
    "        end_art = index_start[i+1]\n",
    "        article = text[start_art:end_art]\n",
    "        articles.append(article)\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = prepare_dataset('de-news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-f7798990cd70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcommon_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Transform each doc into a bag of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "\n",
    "common_dictionary = corpora.Dictionary(articles)\n",
    "# Transform each doc into a bag of words\n",
    "common_corpus = [common_dictionary.doc2bow(a) for a in articles]\n",
    "# This line is the actual training part and might take a few minutes\n",
    "n_topics = 2\n",
    "lda = LdaModel(common_corpus, id2word=common_dictionary, num_topics=n_topics, passes=100)\n",
    "# After training is done, we can check the top words of each topic\n",
    "for k in range(n_topics):\n",
    "\ttop_words = lda.show_topic(k, topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
